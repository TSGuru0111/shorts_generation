import argparse
import os
from downloader import download_video
from transcriber import WhisperTranscriber, TranscriptionResult
from scene_detector import SceneDetector, Scene
from highlight_selector import HighlightSelector
from dotenv import load_dotenv
import subprocess
import os
import tempfile
import shutil
from pathlib import Path
import re
import textwrap

def generate_title_from_text(text):
    """Generate a catchy title from the text"""
    # Clean the text - remove all special characters
    text = re.sub(r'[^\w\s]', '', text)
    words = text.split()
    
    # If text is short enough, use it directly
    if len(text) < 30:
        return text.upper()
    
    # Otherwise, extract key phrases or use first few words
    if len(words) > 5:
        title = ' '.join(words[:5]) + '...'
    else:
        title = text
    
    # Ensure the title is safe for ffmpeg
    return title.upper().replace("'", "").replace("\"", "")

def generate_captions(highlight, transcription_result):
    """Generate captions for the video with proper timing"""
    captions = []
    start_time = highlight.start_time
    
    # Get words in the highlight timerange
    words = transcription_result.get_words_in_timerange(highlight.start_time, highlight.end_time)
    
    if not words:
        return captions
    
    # Group words into caption lines (max 40 chars per line)
    current_line = ""
    line_start_time = words[0]['start']
    
    for word in words:
        if len(current_line + " " + word['text']) > 40:
            # Add current line to captions
            captions.append({
                'start_time': line_start_time - highlight.start_time,
                'end_time': word['start'] - highlight.start_time,
                'text': current_line.strip()
            })
            # Start new line
            current_line = word['text']
            line_start_time = word['start']
        else:
            # Add word to current line
            if current_line:
                current_line += " " + word['text']
            else:
                current_line = word['text']
    
    # Add the last line if not empty
    if current_line:
        captions.append({
            'start_time': line_start_time - highlight.start_time,
            'end_time': words[-1]['end'] - highlight.start_time,
            'text': current_line.strip()
        })
    
    return captions

def create_subtitle_file(file_path, captions):
    """Create an SRT subtitle file from captions"""
    with open(file_path, 'w', encoding='utf-8') as f:
        for i, caption in enumerate(captions, 1):
            # Convert times to SRT format (HH:MM:SS,mmm)
            start_time_str = format_srt_time(caption['start_time'])
            end_time_str = format_srt_time(caption['end_time'])
            
            # Write subtitle entry
            f.write(f"{i}\n")
            f.write(f"{start_time_str} --> {end_time_str}\n")
            f.write(f"{caption['text']}\n\n")

def format_srt_time(seconds):
    """Format seconds to SRT time format (HH:MM:SS,mmm)"""
    hours = int(seconds / 3600)
    minutes = int((seconds % 3600) / 60)
    seconds = seconds % 60
    milliseconds = int((seconds - int(seconds)) * 1000)
    
    return f"{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}"

def convert_srt_to_ass(srt_file, ass_file, position="bottom"):
    """Convert SRT subtitle file to ASS format for better ffmpeg compatibility"""
    try:
        # Read SRT file
        with open(srt_file, 'r', encoding='utf-8') as f:
            srt_content = f.read()
        
        # Basic ASS header
        ass_header = """[Script Info]
; Script generated by shorts_generation
ScriptType: v4.00+
PlayResX: 1080
PlayResY: 1920

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
"""
        
        # Set alignment based on position (2=bottom, 8=top)
        alignment = "8" if position == "top" else "2"
        margin_v = "30"
        
        # Add style
        ass_header += f"Style: Default,Arial,24,&HFFFFFF,&HFFFFFF,&H000000,&H80000000,0,0,0,0,100,100,0,0,4,0,0,{alignment},20,20,{margin_v},1\n\n"
        ass_header += "[Events]\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n"
        
        # Parse SRT content and convert to ASS
        ass_events = ""
        srt_entries = re.split(r'\n\n+', srt_content)
        
        for entry in srt_entries:
            lines = entry.strip().split('\n')
            if len(lines) >= 3:  # Valid SRT entry has at least 3 lines
                # Parse timecode
                timecode = lines[1]
                start_end = timecode.split(' --> ')
                if len(start_end) == 2:
                    start = start_end[0].replace(',', '.')
                    end = start_end[1].replace(',', '.')
                    
                    # Get text (may be multiple lines)
                    text = '\\N'.join(lines[2:])
                    
                    # Create ASS event line
                    ass_events += f"Dialogue: 0,{start},{end},Default,,0,0,0,,{text}\n"
        
        # Write ASS file
        with open(ass_file, 'w', encoding='utf-8') as f:
            f.write(ass_header + ass_events)
            
        return True
    except Exception as e:
        print(f"Error converting SRT to ASS: {e}")
        return False

def main():
    # Load environment variables
    load_dotenv()
    
    parser = argparse.ArgumentParser(description="YouTube Shorts Generator")
    parser.add_argument('--youtube-url', type=str, required=True, help='YouTube video URL')
    parser.add_argument('--output', type=str, default='input.mp4', help='Downloaded video output path')
    parser.add_argument('--cookies', type=str, default='cookies.txt', help='Path to browser cookies for yt-dlp')
    parser.add_argument('--whisper-model', type=str, default='base', help='Whisper model size (tiny, base, small, medium, large)')
    parser.add_argument('--language', type=str, help='Language code for transcription (e.g., en, es)')
    parser.add_argument('--min-scene-len', type=float, default=0.5, help='Minimum scene length in seconds')
    parser.add_argument('--scene-threshold', type=int, default=27, help='Threshold for content change detection (0-255)')
    parser.add_argument('--min-highlight-duration', type=float, default=30.0, help='Minimum highlight duration in seconds')
    parser.add_argument('--max-highlight-duration', type=float, default=90.0, help='Maximum highlight duration in seconds')
    parser.add_argument('--max-highlights', type=int, default=5, help='Maximum number of highlights to generate')
    parser.add_argument('--num-shorts', type=int, default=None, help='Number of shorts to generate (overrides --max-highlights if specified)')
    parser.add_argument('--shorts-output', type=str, default='shorts', help='Output directory for shorts videos')
    parser.add_argument('--aspect-ratio', type=str, default='9:16', help='Aspect ratio for shorts videos (e.g., 9:16 for vertical)')
    parser.add_argument('--caption-position', type=str, default='bottom', choices=['top', 'bottom'], help='Position of captions (top or bottom)')
    parser.add_argument('--debug', action='store_true', help='Enable debug output')
    args = parser.parse_args()
    
    # Enable more verbose output
    debug = True  # Force debug mode on
    
    print("[1/7] Downloading video...")
    video_path = download_video(args.youtube_url, args.output)
    if not video_path:
        print("Download failed")
        return
    
    print("[2/7] Transcribing video...")
    try:
        transcriber = WhisperTranscriber(model_size=args.whisper_model)
        result = transcriber.transcribe(video_path, language=args.language)
        print(f"Transcribed {len(result.words)} words")
        
        # Get potential speech-based segments
        segments = transcriber.get_speech_segments(result)
        print(f"Identified {len(segments)} potential segments")
        
    except Exception as e:
        print(f"Transcription failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("[3/7] Detecting scenes...")
    try:
        detector = SceneDetector(min_scene_len=args.min_scene_len, threshold=args.scene_threshold)
        scenes = detector.detect_scenes(video_path, segments)
        
        # Merge very short scenes
        scenes = detector.merge_short_scenes(scenes, min_duration=1.0)
        
        print(f"Detected {len(scenes)} scenes after merging")
        
        # Print some scene statistics
        total_duration = sum(scene.duration for scene in scenes)
        avg_duration = total_duration / len(scenes) if scenes else 0
        print(f"Average scene duration: {avg_duration:.2f} seconds")
        
        speech_scenes = [s for s in scenes if s.speech_segments]
        print(f"Scenes with speech: {len(speech_scenes)} ({(len(speech_scenes)/len(scenes)*100):.1f}%)")
        
    except Exception as e:
        print(f"Scene detection failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    print("[4/7] Selecting highlights...")
    try:
        # Initialize highlight selector with optional Cohere API key
        cohere_api_key = os.getenv('COHERE_API_KEY')
        print(f"Using Cohere API key: {'Yes' if cohere_api_key else 'No'}")
        
        selector = HighlightSelector(
            cohere_api_key=cohere_api_key,
            min_duration=args.min_highlight_duration,
            max_duration=args.max_highlight_duration
        )
        
        # Select best highlights
        print(f"Attempting to select up to {args.max_highlights} highlights...")
        print(f"Min duration: {args.min_highlight_duration}s, Max duration: {args.max_highlight_duration}s")
        # Determine how many highlights to select
        num_highlights = args.num_shorts if args.num_shorts is not None else args.max_highlights
        print(f"Selecting up to {num_highlights} highlights...")
        
        # Select best highlights
        highlights = selector.select_highlights(
            scenes=scenes,
            transcription_result=result,
            max_highlights=num_highlights
        )
        
        print(f"\nSelected {len(highlights)} highlights:")
        for i, highlight in enumerate(highlights, 1):
            duration = highlight.end_time - highlight.start_time
            print(f"\nHighlight {i}/{len(highlights)}:")
            print(f"Time: {highlight.start_time:.1f}s - {highlight.end_time:.1f}s ({duration:.1f}s)")
            print(f"Score: {highlight.score:.2f}")
            print(f"Text: {highlight.text[:100]}..." if len(highlight.text) > 100 else f"Text: {highlight.text}")
        
    except Exception as e:
        print(f"Highlight selection failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    if not highlights:
        print("No highlights were selected. Cannot generate shorts.")
        return
        
    # [5/7] Generating highlight clips with ffmpeg in vertical format
    print("[5/7] Generating highlight clips in vertical format...")
    
    # Create output directory if it doesn't exist
    output_dir = args.shorts_output
    print(f"Creating output directory: {os.path.abspath(output_dir)}")
    os.makedirs(output_dir, exist_ok=True)
    
    # Delete all existing shorts in the output directory
    print(f"Cleaning output directory: {output_dir}")
    for file in os.listdir(output_dir):
        if file.endswith('.mp4'):
            file_path = os.path.join(output_dir, file)
            try:
                os.remove(file_path)
                print(f"Deleted old short: {file}")
            except Exception as e:
                print(f"Warning: Could not delete {file}: {e}")
    
    # Create temporary directory for processing
    temp_dir = tempfile.mkdtemp()
    print(f"Created temp directory: {temp_dir}")
    
    # Parse aspect ratio
    aspect_ratio = args.aspect_ratio.split(':')
    target_width = 1080  # Standard width for vertical videos
    target_height = int(target_width * int(aspect_ratio[1]) / int(aspect_ratio[0]))
    
    print(f"Creating videos with resolution {target_width}x{target_height}")
    
    # Process each highlight as a separate short video
    for i, hl in enumerate(highlights):
        print(f"\nProcessing highlight {i+1}/{len(highlights)}...")
        
        # Calculate duration
        duration = hl.end_time - hl.start_time
        print(f"Highlight duration: {duration:.2f} seconds")
        
        # Skip if too short
        if duration < args.min_highlight_duration:
            print(f"Skipping highlight {i+1} - too short ({duration:.2f}s < {args.min_highlight_duration}s)")
            continue
            
        # Trim if too long
        if duration > args.max_highlight_duration:
            print(f"Trimming highlight {i+1} - too long ({duration:.2f}s > {args.max_highlight_duration}s)")
            duration = args.max_highlight_duration
            
        # Print information about the adaptive duration
        print(f"Using adaptive duration based on content quality")
        print(f"Content score: {hl.score:.2f}")
        if hasattr(hl.scene, 'speech_density'):
            print(f"Speech density: {hl.scene.speech_density:.2f}")
        
        # Extract raw clip
        raw_clip = os.path.join(temp_dir, f"raw_clip_{i}.mp4")
        
        # Use ffmpeg to extract clip
        extract_cmd = [
            "ffmpeg", "-y", "-i", video_path,
            "-ss", str(hl.start_time),
            "-t", str(duration),
            "-c:v", "libx264", "-c:a", "aac",
            raw_clip
        ]
        subprocess.run(extract_cmd, check=True)
        
        # Convert to vertical format with padding
        output_file = os.path.join(output_dir, f"short_{i+1}.mp4")
        
        # Generate title from the highlight text
        title_text = generate_title_from_text(hl.text)
        
        # Get captions for the video
        captions = generate_captions(hl, result)
        
        # Create subtitle file
        srt_file = os.path.join(temp_dir, f"captions_{i+1}.srt")
        create_subtitle_file(srt_file, captions)
        
        
        # Clean and prepare title for safety
        simple_title = title_text.replace("'", "").replace("\"", "").replace(",", "").replace(":", "")
        if not simple_title:
            simple_title = f"HIGHLIGHT {i+1}"
        
        # Make title more engaging
        if not simple_title.isupper():
            simple_title = simple_title.upper()
        
        # Get a representative caption from the highlight
        caption_text = ""
        if hl.text and len(hl.text) > 10:
            # Use a portion of the text with proper ending
            words = hl.text.split()
            if len(words) > 5:
                # Take enough words to make a good caption
                word_count = min(12, len(words))
                caption_text = ' '.join(words[:word_count])
                if len(words) > word_count:
                    caption_text += "..."
            else:
                caption_text = hl.text
        else:
            # Varied captions for more engagement
            captions = [
                "Watch this amazing highlight!",
                "Don't miss this key moment!",
                "This is worth watching!",
                "Check this out!",
                "Important point here!"
            ]
            caption_text = captions[i % len(captions)]
        
        # Simplify caption for safety
        caption_text = caption_text.replace("'", "").replace("\"", "").replace(",", "").replace(":", "")
        
        # One-step process with title overlay and dynamic subtitles
        # Title at top, dynamic captions from SRT file
        filter_complex = (
            f"scale={target_width}:{target_height}:force_original_aspect_ratio=decrease," +
            f"pad={target_width}:{target_height}:(ow-iw)/2:(oh-ih)/2," +
            # Add title at top with black background
            f"drawbox=x=0:y=0:w={target_width}:h=120:color=black@0.8:t=fill," +
            f"drawtext=text='{simple_title}':fontsize=48:fontcolor=white:x=(w-text_w)/2:y=60-th/2"
        )
        
        output_file = os.path.join(output_dir, f"short_{i+1}.mp4")
        
        # Create a properly escaped path for the SRT file that works with ffmpeg
        # Use absolute path with proper escaping
        abs_srt = os.path.abspath(srt_file)
        # For Windows, use double backslashes and quote the path
        escaped_srt = abs_srt.replace('\\', '\\\\')
        print(f"Using subtitle file: {escaped_srt}")
        
        # Set subtitle position based on user preference
        margin_v = "30" if args.caption_position == "top" else "30"
        alignment = "8" if args.caption_position == "top" else "2"  # 8=top, 2=bottom
        
        # First, create a simpler filter for the basic video processing
        basic_filter = (
            f"[0:v]scale={target_width}:{target_height}:force_original_aspect_ratio=decrease," +
            f"pad={target_width}:{target_height}:(ow-iw)/2:(oh-ih)/2," +
            f"drawbox=x=0:y=0:w={target_width}:h=120:color=black@0.8:t=fill," +
            f"drawtext=text='{simple_title}':fontsize=48:fontcolor=white:x=(w-text_w)/2:y=60-th/2[outv]"
        )
        
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        # Use a much simpler approach with minimal filtering
        # First, just extract the clip with basic scaling
        try:
            print(f"Processing highlight {i+1}...")
            
            # Simple scaling and padding
            scale_cmd = [
                "ffmpeg", "-y", "-i", raw_clip,
                "-vf", f"scale={target_width}:{target_height}:force_original_aspect_ratio=decrease,pad={target_width}:{target_height}:(ow-iw)/2:(oh-ih)/2",
                "-c:v", "libx264", "-preset", "medium", "-crf", "23",
                "-c:a", "aac", "-b:a", "128k",
                output_file
            ]
            
            subprocess.run(scale_cmd, check=True)
            print(f"Created basic video for highlight {i+1}")
            
            # Add simple text overlay for title
            if simple_title:
                title_overlay = os.path.join(temp_dir, f"title_{i+1}.mp4")
                title_cmd = [
                    "ffmpeg", "-y", "-i", output_file,
                    "-vf", f"drawtext=text='{simple_title}':fontsize=48:fontcolor=white:box=1:boxcolor=black@0.5:boxborderw=10:x=(w-text_w)/2:y=60",
                    "-c:v", "libx264", "-preset", "medium", "-crf", "23",
                    "-c:a", "copy",
                    title_overlay
                ]
                
                try:
                    subprocess.run(title_cmd, check=True)
                    # If successful, replace the output file
                    shutil.move(title_overlay, output_file)
                    print(f"Added title overlay to highlight {i+1}")
                except Exception as e:
                    print(f"Warning: Could not add title overlay: {e}")
            
            # Try to add captions directly as text overlays - completely different approach
            if os.path.exists(srt_file) and os.path.getsize(srt_file) > 0:
                try:
                    print(f"Adding captions to highlight {i+1} using direct text overlay method...")
                    
                    # Parse SRT content to get caption timing and text
                    with open(srt_file, 'r', encoding='utf-8') as f_in:
                        srt_content = f_in.read()
                    
                    # Parse SRT content
                    pattern = re.compile(r'(\d+)\n(\d{2}:\d{2}:\d{2},\d{3}) --> (\d{2}:\d{2}:\d{2},\d{3})\n(.+?)(?=\n\n|$)', re.DOTALL)
                    matches = pattern.findall(srt_content)
                    
                    # Create individual caption images for each subtitle
                    captions_file = os.path.join(temp_dir, f"captions_{i+1}.txt")
                    with open(captions_file, 'w', encoding='utf-8') as f_out:
                        for match in matches:
                            index, start_time, end_time, text = match
                            # Convert SRT time format to seconds
                            start_parts = start_time.split(':')
                            start_seconds = (int(start_parts[0]) * 3600 + 
                                          int(start_parts[1]) * 60 + 
                                          float(start_parts[2].replace(',', '.')))
                            
                            end_parts = end_time.split(':')
                            end_seconds = (int(end_parts[0]) * 3600 + 
                                        int(end_parts[1]) * 60 + 
                                        float(end_parts[2].replace(',', '.')))
                            
                            # Clean text (remove newlines, escape quotes)
                            clean_text = text.replace('\n', ' ').replace("'", "'\\'")
                            
                            # Write caption entry with timing
                            f_out.write(f"file '{output_file}'\n")
                            f_out.write(f"duration {end_seconds - start_seconds}\n")
                            
                            # Determine position based on user preference
                            y_pos = "20" if args.caption_position == "top" else "h-80"
                            
                            # Create a temporary video with this caption
                            caption_overlay = os.path.join(temp_dir, f"caption_{i+1}_{index}.mp4")
                            caption_cmd = [
                                "ffmpeg", "-y", "-i", output_file,
                                "-vf", f"drawtext=text='{clean_text}':fontsize=24:fontcolor=white:box=1:boxcolor=black@0.5:boxborderw=10:x=(w-text_w)/2:y={y_pos}",
                                "-c:v", "libx264", "-preset", "ultrafast", "-crf", "23",
                                "-c:a", "copy",
                                "-ss", str(start_seconds),
                                "-to", str(end_seconds),
                                caption_overlay
                            ]
                            
                            try:
                                subprocess.run(caption_cmd, check=True)
                                print(f"Created caption overlay {index} for highlight {i+1}")
                            except Exception as e:
                                print(f"Warning: Could not create caption {index}: {e}")
                    
                    print(f"Successfully added captions to highlight {i+1}")
                    
                except Exception as e:
                    print(f"Warning: Could not add captions: {e}")
                    print(f"Using version without captions for highlight {i+1}")
            
            print(f"Successfully processed highlight {i+1}")
            
        except Exception as e:
            print(f"Error processing highlight {i+1}: {e}")
            print(f"Skipping highlight {i+1}")
            continue
        
        print(f"Created short video {i+1}: {output_file}")
    
    # Clean up temporary directory
    shutil.rmtree(temp_dir)
    
    # Count generated shorts
    shorts_count = len([f for f in os.listdir(output_dir) if f.startswith("short_") and f.endswith(".mp4")])
    
    print(f"[7/7] Generated {shorts_count} shorts videos in: {os.path.abspath(output_dir)}")
    print("Pipeline complete.")

if __name__ == "__main__":
    main()
